{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "02-SparkSQL-DataFrames.workingme.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FVLq8C5prlE6",
        "NYHfc27QrlFJ",
        "83DICaI0rlFt",
        "eAM4C1k-rlGH"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rasoco/master_data_science/blob/master/Big%20Data/Clase%202/02_SparkSQL_DataFrames_workingme.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T673XyrrlCv",
        "colab_type": "text"
      },
      "source": [
        "# SparkSQL and DataFrames \n",
        "\n",
        "<a href = \"http://yogen.io\"><img src=\"http://yogen.io/assets/logo.svg\" alt=\"yogen\" style=\"width: 200px; float: right;\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0wLLipPrlCw",
        "colab_type": "text"
      },
      "source": [
        "## RDDs, DataSets, and DataFrames\n",
        "\n",
        "RDDs are the original interface for Spark programming.\n",
        "\n",
        "DataFrames were introduced in 1.3\n",
        "\n",
        "Datasets were introduced in 1.6, and unified with DataFrames in 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji2PDn4trlCy",
        "colab_type": "text"
      },
      "source": [
        "### Advantages of DataFrames:\n",
        "\n",
        "from https://www.datacamp.com/community/tutorials/apache-spark-python:\n",
        "\n",
        "> More specifically, the performance improvements are due to two things, which you’ll often come across when you’re reading up DataFrames: custom memory management (project Tungsten), which will make sure that your Spark jobs much faster given CPU constraints, and optimized execution plans (Catalyst optimizer), of which the logical plan of the DataFrame is a part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "442BTDTBrlCz",
        "colab_type": "text"
      },
      "source": [
        "## SparkSQL and DataFrames \n",
        "\n",
        "\n",
        "pyspark does not have the Dataset API, which is available only if you use Spark from a statically typed language: Scala or Java.\n",
        "\n",
        "From https://spark.apache.org/docs/2.4.4/sql-programming-guide.html\n",
        "\n",
        "> A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset&lt;Row> to represent a DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO9l4X4xrlC0",
        "colab_type": "text"
      },
      "source": [
        "### The pyspark.sql module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTeepej6rlC1",
        "colab_type": "text"
      },
      "source": [
        "Important classes of Spark SQL and DataFrames:\n",
        "\n",
        "* `pyspark.sql.SparkSession` Main entry point for DataFrame and SQL functionality.\n",
        "\n",
        "* `pyspark.sql.DataFrame` A distributed collection of data grouped into named columns.\n",
        "\n",
        "* `pyspark.sql.Column` A column expression in a DataFrame.\n",
        "\n",
        "* `pyspark.sql.Row` A row of data in a DataFrame.\n",
        "\n",
        "* `pyspark.sql.GroupedData` Aggregation methods, returned by DataFrame.groupBy().\n",
        "\n",
        "* `pyspark.sql.DataFrameNaFunctions` Methods for handling missing data (null values).\n",
        "\n",
        "* `pyspark.sql.DataFrameStatFunctions` Methods for statistics functionality.\n",
        "\n",
        "* `pyspark.sql.functions` List of built-in functions available for DataFrame.\n",
        "\n",
        "* `pyspark.sql.types` List of data types available.\n",
        "\n",
        "* `pyspark.sql.Window` For working with window functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4I8FqULrlC1",
        "colab_type": "text"
      },
      "source": [
        "http://spark.apache.org/docs/2.4.4/api/python/pyspark.sql.html\n",
        "\n",
        "https://spark.apache.org/docs/2.4.4/sql-programming-guide.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOb8vlnsrlC2",
        "colab_type": "text"
      },
      "source": [
        "## SparkSession\n",
        "\n",
        "The traditional way to interact with Spark is the SparkContext. In the notebooks we get that from the pyspark driver.\n",
        "\n",
        "From 2.0 we can use SparkSession to replace SparkConf, SparkContext and SQLContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1aaPt9MrlC2",
        "colab_type": "text"
      },
      "source": [
        "### If you are running this notebook in Google Colab\n",
        "\n",
        "Copy the following to a code cell and run it. It will install and set up Spark for you.\n",
        "\n",
        "```python\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.uvigo.es/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!tar -xf spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
        "findspark.init()\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Miw1QurlC3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9dbdc6d9-81a8-4ddb-9a33-ce30ba32f4aa"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.uvigo.es/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!tar -xf spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark pyspark==2.4.6\n",
        " \n",
        "import os\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        " \n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
        "findspark.init()\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.uvigo.es/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!tar -xf spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark pyspark==2.4.6\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 218.4MB 62kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 47.3MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-JUfkN3rlC7",
        "colab_type": "text"
      },
      "source": [
        "#### Passing other options to spark session:\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlSR0uz3rlC8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "5448c2f3-a1bb-4101-fd0c-3f86d9eaf885"
      },
      "source": [
        "spark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://21c206dd9641:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.6</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ff4fa981198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8juvPKPuQUG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "b49187a3-9a0c-45fd-a513-6483cdb9caa0"
      },
      "source": [
        "spark.sparkContext"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://21c206dd9641:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.6</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq39qag-rlDA",
        "colab_type": "text"
      },
      "source": [
        "We can check option values in the resulting session like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-kpHwtvrlDB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "7bb377b1-56c6-4c26-f67c-f0b633c9088e"
      },
      "source": [
        "spark.sparkContext.getConf().getAll()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.driver.host', '21c206dd9641'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.app.id', 'local-1593797212585'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.driver.port', '37007'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.app.name', 'pyspark-shell')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHELojoXrlDE",
        "colab_type": "text"
      },
      "source": [
        "### Creating DataFrames\n",
        "\n",
        "SparkSession.createDataFrame: from an RDD, a list or a pandas.DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Euq8mHDkrlDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "592ececc-ef9a-4f99-bf9d-b6428e033d26"
      },
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "n = 20\n",
        "races = random.choices(['elf', 'hobbit', 'orc'], k=n)\n",
        "creatures = [(id_, race) for id_, race in zip(range(n), races)] #crear lista de tuplas\n",
        "creatures"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'hobbit'),\n",
              " (1, 'elf'),\n",
              " (2, 'elf'),\n",
              " (3, 'elf'),\n",
              " (4, 'orc'),\n",
              " (5, 'orc'),\n",
              " (6, 'orc'),\n",
              " (7, 'elf'),\n",
              " (8, 'hobbit'),\n",
              " (9, 'elf'),\n",
              " (10, 'elf'),\n",
              " (11, 'hobbit'),\n",
              " (12, 'elf'),\n",
              " (13, 'elf'),\n",
              " (14, 'hobbit'),\n",
              " (15, 'hobbit'),\n",
              " (16, 'elf'),\n",
              " (17, 'hobbit'),\n",
              " (18, 'orc'),\n",
              " (19, 'elf')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZQhNkCmw276",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "476cf740-a9a2-4910-db78-532f94d0cf37"
      },
      "source": [
        "df = spark.createDataFrame(creatures)\n",
        "df # columna con un nombre autogenerada _1 y _2 y un tipo"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[_1: bigint, _2: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t33H2lVcxFW1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "1ecb2ac5-3762-4fb1-a372-b6613c38b98c"
      },
      "source": [
        "df.take(5) #elementos son filas, tupla con nombres "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(_1=0, _2='hobbit'),\n",
              " Row(_1=1, _2='elf'),\n",
              " Row(_1=2, _2='elf'),\n",
              " Row(_1=3, _2='elf'),\n",
              " Row(_1=4, _2='orc')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT8Rl8fRxO_6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80a53d08-524b-458d-8d0f-26f5287909e0"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "Row(id_=4, race='elf')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(id_=4, race='elf')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z12a9Lldxocc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "695a7a68-1af9-4776-98f9-245d8f28ec77"
      },
      "source": [
        "df = spark.createDataFrame(creatures, schema=['id', 'race']) #dar nombre a las columnas\n",
        "df"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, race: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cF1Y-U5lx35B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "db0db46b-f7a0-4c63-920b-7f3eadcf5a70"
      },
      "source": [
        "df.show(5)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "| id|  race|\n",
            "+---+------+\n",
            "|  0|hobbit|\n",
            "|  1|   elf|\n",
            "|  2|   elf|\n",
            "|  3|   elf|\n",
            "|  4|   orc|\n",
            "+---+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6zJdSwZx767",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8dd63419-aef2-4426-dba8-dc6788da22f4"
      },
      "source": [
        "df.printSchema() #campo que sea obligatorio (nullable)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- race: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2DzV863rlDJ",
        "colab_type": "text"
      },
      "source": [
        "### Creating DataFrames\n",
        "\n",
        "* From RDDs\n",
        "* from Hive tables\n",
        "* From Spark sources: parquet (default), json, jdbc, orc, libsvm, csv, text\n",
        "* Is inmutable\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGR1EYlgyo6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "4cba20ee-dbef-43b4-d7ed-548cb0168c43"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJAlftiRrlDJ",
        "colab_type": "text"
      },
      "source": [
        "#### From RDDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeCwYcacrlDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdd = spark.sparkContext.textFile('/content/drive/My Drive/BigData Spark/Clase 1/coupon150720.csv.gz')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UeUFzI7zVqA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "46b600f4-94c7-4672-d207-7479e99bbfd1"
      },
      "source": [
        "rdd.take(3)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0',\n",
              " '79062005698500,2,AUH,CDG,9W,9W,84.34,USD,1,H,H,6120,150905,OK,IAF0',\n",
              " '79062005924069,1,CJB,MAA,9W,9W,60.0,USD,1,H,H,2768,150721,OK,IAA0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wxk3-Pfz9Bt",
        "colab_type": "text"
      },
      "source": [
        "Si aplicamos map vamos a tener un rdd anidado saca, son agrupación en lista\n",
        "\n",
        "si queremos romper la agrupación en línea para ello utilizaremos flatMap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHR-MLGhzk0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_lines = rdd.map(lambda line: line.split(',')).take(3)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT9yA0Gk0Wqc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "45d9cfb5-3c41-465e-e4a2-e7314a41c152"
      },
      "source": [
        "spark.createDataFrame(split_lines)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[_1: string, _2: string, _3: string, _4: string, _5: string, _6: string, _7: string, _8: string, _9: string, _10: string, _11: string, _12: string, _13: string, _14: string, _15: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrM7StCQrlDO",
        "colab_type": "text"
      },
      "source": [
        "### Inferring and specifying schemas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmhCJFilrlDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvfcZKZtrlDS",
        "colab_type": "text"
      },
      "source": [
        "#### Fully specifying a schema\n",
        "\n",
        "We need to create a `StructType` composed of `StructField`s. each of those specifies afiled with name, type and `nullable` properties. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz2GPPhzrlDT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "283254c5-c7a5-4ee0-9ab1-c45e86a88807"
      },
      "source": [
        "from pyspark.sql import types\n",
        "types.BooleanType()\n",
        "\n",
        "schema = types.StructType([types.StructField('id', types.IntegerType(), False), types.StructField('race', types.StringType())])\n",
        "df = spark.createDataFrame(creatures, schema=schema)\n",
        "df.printSchema() #no pueden ver valores nulos"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: integer (nullable = false)\n",
            " |-- race: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dISXFtiqrlDW",
        "colab_type": "text"
      },
      "source": [
        "#### From csv files\n",
        "\n",
        "We can either read them directly into dataframes or read them as RDDs and transform that into a DataFrame. This second way will be very useful if we have unstructured data like web server logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pCsLefyrlDW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "90f72782-4e29-4bf8-9de8-0b25eb1deb65"
      },
      "source": [
        "spark.read.csv('/content/drive/My Drive/BigData Spark/Clase 1/coupon150720.csv.gz', inferSchema=True)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[_c0: bigint, _c1: int, _c2: string, _c3: string, _c4: string, _c5: string, _c6: double, _c7: string, _c8: int, _c9: string, _c10: string, _c11: string, _c12: int, _c13: string, _c14: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrVmANNW4cWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coupons = spark.sql('''SELECT\n",
        "              CAST(_c0 AS BIGINT) as tkt_number,\n",
        "              CAST(_c1 AS INT) as cpn_number,\n",
        "              _c2 as origin,\n",
        "              _c3 as dest,\n",
        "              _c4 as carrier,\n",
        "              CAST(_c6 AS FLOAT) as amount\n",
        "              FROM csv. `/content/drive/My Drive/BigData Spark/Clase 1/coupon150720.csv.gz`''') "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACNTUB_drlDa",
        "colab_type": "text"
      },
      "source": [
        "#### From other types of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVt-dhuprlDb",
        "colab_type": "text"
      },
      "source": [
        "Apache Parquet is a free and open-source column-oriented data store of the Apache Hadoop ecosystem. It is similar to the other columnar storage file formats available in Hadoop namely RCFile and Optimized RCFile. It is compatible with most of the data processing frameworks in the Hadoop environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKnlV2cWrlDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9d22cc0-2eb3-4b5e-b25d-bbee747ddbb6"
      },
      "source": [
        "spark.read.jdbc #(java database conexion) conectar un programa a una base de datos"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrameReader.jdbc of <pyspark.sql.readwriter.DataFrameReader object at 0x7ff4f961cdd8>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JII29WNxrlDf",
        "colab_type": "text"
      },
      "source": [
        "### Basic operations with DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgP973m6rlDf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "25ee9287-4503-4793-eabd-a183b804e13b"
      },
      "source": [
        "df.show(5) #muestra los n primeros"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "| _1|    _2|\n",
            "+---+------+\n",
            "|  0|hobbit|\n",
            "|  1|   elf|\n",
            "|  2|   elf|\n",
            "|  3|   elf|\n",
            "|  4|   orc|\n",
            "+---+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9MRUdXf7jee",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59dffe7f-2950-4109-e21e-a92b7426a5af"
      },
      "source": [
        "df.take(3) #genera lista"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(_1=0, _2='hobbit'), Row(_1=1, _2='elf'), Row(_1=2, _2='elf')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7t9Lv24rlDj",
        "colab_type": "text"
      },
      "source": [
        "### Filtering and selecting\n",
        "\n",
        "Syntax inspired in SQL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DPqmgqCrlDk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c71a41e-a63b-4db2-b43b-1358b7aa7934"
      },
      "source": [
        "df.select('id') #data frame con la columna id seria _1"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQEIrA6DrlDn",
        "colab_type": "text"
      },
      "source": [
        "If we want to filter, we will need to build an instance of `Column`, using square bracket notation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTK5y5_UrlDo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "64c68765-c0c8-4acb-db78-663dbd6b6974"
      },
      "source": [
        "df['id'].show()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-1f772950feca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "raises-exception"
        ],
        "id": "t3QF_P3XrlDs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb1be823-15a9-4593-f727-5dbe99904623"
      },
      "source": [
        "df.select(df['id'])"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e74morR88jQe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab03d7a7-7ce8-4181-e818-3f8becd1f8f4"
      },
      "source": [
        "df.filter(df['id'] < 5)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, race: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI44JctJ8jly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c315ee57-95e2-4e49-c752-1a9ee3749fee"
      },
      "source": [
        "df['id'] < 5 #otra columna tipo booleano"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<b'(id < 5)'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbCznhO2rlDr",
        "colab_type": "text"
      },
      "source": [
        "That's because a comparison between str and int will error out, so spark will not even get the chance to infer to which column we are referring."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlGkJ0AR9CfN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "cf7858ea-95b4-4ca6-acdf-de321f2b4390"
      },
      "source": [
        "df.filter('id' < 5) #no se pueden comparar string con integer"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-48db9ba8859e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#no se pueden comparar string con integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_yKz65drlDw",
        "colab_type": "text"
      },
      "source": [
        "`where` is exactly synonimous with `filter`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxZTlr5FrlDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df.where or df. filter es igual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An4XtJ2p9YPf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c032a458-2701-4a86-895a-59e861fde124"
      },
      "source": [
        "df.where(df['id'] < 5)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, race: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAPIarrmrlDz",
        "colab_type": "text"
      },
      "source": [
        "A column is quite different to a Pandas Series. It is just a reference to a column, and can only be used to construct sparkSQL expressions (select, where...). It can't be collected or taken as a one-dimensional sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "raises-exception"
        ],
        "id": "LGi_SQhArlDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['race'].show() # no puedo pensar como en las series del dataframe de pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f3CGSxUrlD8",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "Extract all employee ids which correspond to orcs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY-WyRVbrlD9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "d2eea44a-83a1-4859-9e00-1079e7e92900"
      },
      "source": [
        "df.filter(df['race'] == 'orc').select('id').show() "
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  4|\n",
            "|  5|\n",
            "|  6|\n",
            "| 18|\n",
            "+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMu9fdlIrlD_",
        "colab_type": "text"
      },
      "source": [
        "### Adding columns\n",
        "\n",
        "Dataframes are immutable, since they are built on top of RDDs, so we can not assign to them. We need to create new DataFrames with the appropriate columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "raises-exception"
        ],
        "id": "c8_OOz5trlEA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "e36ce85c-c532-43ec-87a9-8ef7f6392a74"
      },
      "source": [
        "df2 = df.withColumn('square', df['id'] ** 2)\n",
        "df2.show(5)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+------+\n",
            "| id|  race|square|\n",
            "+---+------+------+\n",
            "|  0|hobbit|   0.0|\n",
            "|  1|   elf|   1.0|\n",
            "|  2|   elf|   4.0|\n",
            "|  3|   elf|   9.0|\n",
            "|  4|   orc|  16.0|\n",
            "+---+------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw9dMZYFrlED",
        "colab_type": "text"
      },
      "source": [
        "### User defined functions\n",
        "\n",
        "There are many useful functions in pyspark.sql.functions. These work on columns, that is, they are vectorial.\n",
        "\n",
        "We can write User Defined Functions (`udf`s), which allow us to \"vectorize\" operations: write a standard function to process single elements, then build a udf with that that works on columns in a DataFrame, like a SQL function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrWmCWKQrlED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93e2c97a-9c04-4622-b45f-521f49396514"
      },
      "source": [
        "df2['id'] + df2['square'] "
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<b'(id + square)'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSTFkhWsA8y0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08ae04d8-b7c4-4368-894e-6c245a13ced9"
      },
      "source": [
        "import math\n",
        "\n",
        "math.log1p(1)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6931471805599453"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDF7F8oEBClv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "9e04e1f3-98ad-496b-e2f6-49d5dcfc4a3e"
      },
      "source": [
        "math.log1p(df['id']) #sabe manejar numeros no columnas"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-0d23a3767f39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: must be real number, not Column"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o92NzLNVrlEK",
        "colab_type": "text"
      },
      "source": [
        "This errors out because \n",
        "\n",
        "```python\n",
        "math.log1p\n",
        "```\n",
        "\n",
        "is not a udf: it doesn't know how to work with strings or Column objects:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "raises-exception"
        ],
        "id": "5nf97a1brlEK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "b1abb7cd-04ea-4fba-96db-ed43661c4500"
      },
      "source": [
        "from pyspark.sql import functions #para operar con columnas \n",
        " \n",
        "df.select(functions.cos(df['id'])).show()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|             COS(id)|\n",
            "+--------------------+\n",
            "|                 1.0|\n",
            "|  0.5403023058681398|\n",
            "| -0.4161468365471424|\n",
            "| -0.9899924966004454|\n",
            "| -0.6536436208636119|\n",
            "| 0.28366218546322625|\n",
            "|  0.9601702866503661|\n",
            "|  0.7539022543433046|\n",
            "|-0.14550003380861354|\n",
            "| -0.9111302618846769|\n",
            "| -0.8390715290764524|\n",
            "|0.004425697988050785|\n",
            "|  0.8438539587324921|\n",
            "|  0.9074467814501962|\n",
            "|  0.1367372182078336|\n",
            "| -0.7596879128588213|\n",
            "| -0.9576594803233847|\n",
            "|-0.27516333805159693|\n",
            "|  0.6603167082440802|\n",
            "|  0.9887046181866692|\n",
            "+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VCaGwLIrlEO",
        "colab_type": "text"
      },
      "source": [
        "But we can transform it into a udf:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TqtpJFcrlEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40ee6560-84eb-4216-a6db-594d9a92ef51"
      },
      "source": [
        "my_udf = functions.udf(math.log1p) \n",
        "my_udf # es una funcion que come una funcion y devuelve una funcion"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function math.log1p>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2KheRAbCGC3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "7c7b1ef0-40f8-4743-9302-f3ec07c1e547"
      },
      "source": [
        "df.select(my_udf(df['id'])).show()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+\n",
            "|         log1p(id)|\n",
            "+------------------+\n",
            "|               0.0|\n",
            "|0.6931471805599453|\n",
            "|1.0986122886681096|\n",
            "|1.3862943611198906|\n",
            "|1.6094379124341003|\n",
            "| 1.791759469228055|\n",
            "|1.9459101490553132|\n",
            "|2.0794415416798357|\n",
            "|2.1972245773362196|\n",
            "| 2.302585092994046|\n",
            "|2.3978952727983707|\n",
            "|2.4849066497880004|\n",
            "|2.5649493574615367|\n",
            "| 2.639057329615259|\n",
            "|  2.70805020110221|\n",
            "| 2.772588722239781|\n",
            "| 2.833213344056216|\n",
            "|2.8903717578961645|\n",
            "|2.9444389791664403|\n",
            "| 2.995732273553991|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lORcTi94rlES",
        "colab_type": "text"
      },
      "source": [
        "We can do the same with any function we dream up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1UZytFDrlET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "e21d9f6e-f167-433e-a797-49a05b7ae89e"
      },
      "source": [
        "last_2 = functions.udf(lambda word: word[-2:])\n",
        "df.select(last_2('race')).show(5)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+\n",
            "|<lambda>(race)|\n",
            "+--------------+\n",
            "|            it|\n",
            "|            lf|\n",
            "|            lf|\n",
            "|            lf|\n",
            "|            rc|\n",
            "+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqLrFtTyrlEW",
        "colab_type": "text"
      },
      "source": [
        "If we want the resulting columns to be of a particular type, we need to specify the return type. This is because in Python return types can not be inferred."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVnc1qDErlEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "4a61ec0e-83d8-474d-a043-23075b5137cd"
      },
      "source": [
        "df.select(my_udf(df['id'])).printSchema() # Es tipo string "
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- log1p(id): string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DnFZdf9rlEZ",
        "colab_type": "text"
      },
      "source": [
        "Think about this function: what is its return type?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tCytpZvrlEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4f_FBbFrlEb",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise: \n",
        "\n",
        "Create a 'hitpoints' field in our df. make it 30000 for halflings, 40000 for elves and 70000 for orcs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qLtLrEUrlEc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2025e121-ebe9-4b30-a18a-75fa6335cbd5"
      },
      "source": [
        " from pyspark.sql import functions \n",
        "\n",
        "def hitpoints(race):\n",
        "    reference = {'elf': 40000, 'orc' : 70000, 'hobbit' : 30000}\n",
        "    return reference[race]\n",
        "\n",
        "hitpoints('hobbit')"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2DOEbA2G1MH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27d3b012-e882-410a-8e76-0206644cdd48"
      },
      "source": [
        "hp_udf = functions.udf(hitpoints, types.IntegerType())\n",
        "hp_udf(df['race'])"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<b'hitpoints(race)'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZz4Ozr8HCRb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "836af7a3-8950-4573-81ff-71c6e1aa0347"
      },
      "source": [
        "df3 = df.withColumn('hp', hp_udf('race')) #añado la columna hp, aplico la variable hp_udf que tiene una función\n",
        "df3.show()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+-----+\n",
            "| id|  race|   hp|\n",
            "+---+------+-----+\n",
            "|  0|hobbit|30000|\n",
            "|  1|   elf|40000|\n",
            "|  2|   elf|40000|\n",
            "|  3|   elf|40000|\n",
            "|  4|   orc|70000|\n",
            "|  5|   orc|70000|\n",
            "|  6|   orc|70000|\n",
            "|  7|   elf|40000|\n",
            "|  8|hobbit|30000|\n",
            "|  9|   elf|40000|\n",
            "| 10|   elf|40000|\n",
            "| 11|hobbit|30000|\n",
            "| 12|   elf|40000|\n",
            "| 13|   elf|40000|\n",
            "| 14|hobbit|30000|\n",
            "| 15|hobbit|30000|\n",
            "| 16|   elf|40000|\n",
            "| 17|hobbit|30000|\n",
            "| 18|   orc|70000|\n",
            "| 19|   elf|40000|\n",
            "+---+------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi3GExM7rlEf",
        "colab_type": "text"
      },
      "source": [
        "If we have a column that is not the desired type, we can convert it with `cast`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBqGzlC3rlEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlCeOtZerlEh",
        "colab_type": "text"
      },
      "source": [
        "### Summary statistics\n",
        "\n",
        "https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V38o70jKrlEh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e6fc4d28-7a8d-42e0-cb35-f1ce43930531"
      },
      "source": [
        "df3.stat.cov('id', 'hp')"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-12105.263157894737"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27UXN9XVrlEj",
        "colab_type": "text"
      },
      "source": [
        "### .crosstab()\n",
        "\n",
        "Crosstab returns the contingency table for two columns, as a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE-XTJr6rlEl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "cdfcda66-44b0-4c89-ece7-27db5e8b46f0"
      },
      "source": [
        "land = functions.udf(lambda: random.choice(['gondor', 'rohan']) ) # a partir de nada creo aleatoriamente\n",
        "\n",
        "df4 = df3.withColumn('land', land())\n",
        "df4.show()\n",
        "df4.cache().show() #para que no cambie el número aleaotorio"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+-----+------+\n",
            "| id|  race|   hp|  land|\n",
            "+---+------+-----+------+\n",
            "|  0|hobbit|30000| rohan|\n",
            "|  1|   elf|40000| rohan|\n",
            "|  2|   elf|40000| rohan|\n",
            "|  3|   elf|40000| rohan|\n",
            "|  4|   orc|70000|gondor|\n",
            "|  5|   orc|70000|gondor|\n",
            "|  6|   orc|70000| rohan|\n",
            "|  7|   elf|40000|gondor|\n",
            "|  8|hobbit|30000|gondor|\n",
            "|  9|   elf|40000|gondor|\n",
            "| 10|   elf|40000|gondor|\n",
            "| 11|hobbit|30000| rohan|\n",
            "| 12|   elf|40000| rohan|\n",
            "| 13|   elf|40000| rohan|\n",
            "| 14|hobbit|30000| rohan|\n",
            "| 15|hobbit|30000|gondor|\n",
            "| 16|   elf|40000| rohan|\n",
            "| 17|hobbit|30000|gondor|\n",
            "| 18|   orc|70000|gondor|\n",
            "| 19|   elf|40000| rohan|\n",
            "+---+------+-----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNTIhLrsQvnC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "eecb5d7e-6239-4229-c3ff-b194de4f7835"
      },
      "source": [
        "df4.crosstab('race', 'land').show()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+------+-----+\n",
            "|race_land|gondor|rohan|\n",
            "+---------+------+-----+\n",
            "|   hobbit|     4|    2|\n",
            "|      orc|     3|    1|\n",
            "|      elf|     3|    7|\n",
            "+---------+------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wM31iHIrlEp",
        "colab_type": "text"
      },
      "source": [
        "### Grouping\n",
        "\n",
        "Grouping works very similarly to Pandas: executing groupby (or groupBy) on a DataFrame will return an object (a GroupedData) that can then be aggregated to obtain the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GNI4zbfrlEp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50a336a3-a038-4bd1-cc27-c558d62ee4b6"
      },
      "source": [
        "gd = df4.groupBy('land')\n",
        "gd"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.group.GroupedData at 0x7ff4f952e2e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cX8YpVerlEs",
        "colab_type": "text"
      },
      "source": [
        "GroupedData has several aggregation functions defined:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yND9Q6iKrlEs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4ffd2307-cfda-40c2-8040-788f2b39bc87"
      },
      "source": [
        "gd.sum('hp').show()"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-------+\n",
            "|  land|sum(hp)|\n",
            "+------+-------+\n",
            "| rohan| 520000|\n",
            "|gondor| 340000|\n",
            "+------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5AkALkOrlEv",
        "colab_type": "text"
      },
      "source": [
        "We can do several aggregations in a single step, with a number of different syntaxes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edYxwLW7rlEw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "06d3593c-c831-4ff2-cd9e-fae8a967c5d8"
      },
      "source": [
        "gd.agg({'hp' : 'mean', 'id' : 'count'}).show()"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-------+---------+\n",
            "|  land|avg(hp)|count(id)|\n",
            "+------+-------+---------+\n",
            "| rohan|45000.0|       10|\n",
            "|gondor|41000.0|       10|\n",
            "+------+-------+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yClhpuTONP4M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c22e29b8-30c2-45c1-8e1c-c7fe558ab45b"
      },
      "source": [
        "gd.agg(functions.mean('hp'),\n",
        "       functions.count('id'),\n",
        "       functions.mean('id')).show()"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-------+---------+-------+\n",
            "|  land|avg(hp)|count(id)|avg(id)|\n",
            "+------+-------+---------+-------+\n",
            "| rohan|41000.0|       10|    7.4|\n",
            "|gondor|45000.0|       10|   11.6|\n",
            "+------+-------+---------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z892uUJ3NYqZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "cbc92e43-4c7e-4ff5-953d-4ee9b495b163"
      },
      "source": [
        "df4.groupby(df4['id'] < 5).mean('hp').show()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+------------------+\n",
            "|(id < 5)|           avg(hp)|\n",
            "+--------+------------------+\n",
            "|    true|           44000.0|\n",
            "|   false|42666.666666666664|\n",
            "+--------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfFP4VrsrlEz",
        "colab_type": "text"
      },
      "source": [
        "### Intersections\n",
        "\n",
        "Ver much like SQL joins. We can specify the columns and the join method (left, right, inner, outer) or we can let Spark infer them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9cSQScNrlEz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4811bdc7-0bfe-4ee5-90fe-4ececb4a7310"
      },
      "source": [
        "result = gd.agg(functions.mean('hp'),\n",
        "       functions.count('id'),\n",
        "       functions.mean('id'))\n",
        "result.show()\n",
        "\n"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------------------+---------+-----------------+\n",
            "|  land|           avg(hp)|count(id)|          avg(id)|\n",
            "+------+------------------+---------+-----------------+\n",
            "| rohan|           45000.0|        8|            10.75|\n",
            "|gondor|41666.666666666664|       12|8.666666666666666|\n",
            "+------+------------------+---------+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmzDZox5RPlZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7eafdb2f-bf62-4c12-8df3-6ec93296587c"
      },
      "source": [
        "df4.join(result).show()"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1343.showString.\n: org.apache.spark.sql.AnalysisException: Detected implicit cartesian product for INNER join between logical plans\nProject [id#280L, race#281, hitpoints(race#281) AS hp#381, <lambda>() AS land#411]\n+- LogicalRDD [id#280L, race#281], false\nand\nAggregate [land#666], [land#666, avg(cast(hp#381 as bigint)) AS avg(hp)#634, count(id#280L) AS count(id)#635L, avg(id#280L) AS avg(id)#636]\n+- Project [id#280L, hitpoints(race#281) AS hp#381, <lambda>() AS land#666]\n   +- LogicalRDD [id#280L, race#281], false\nJoin condition is missing or trivial.\nEither: use the CROSS JOIN syntax to allow cartesian products between these\nrelations, or: enable implicit cartesian products by setting the configuration\nvariable spark.sql.crossJoin.enabled=true;\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$22.applyOrElse(Optimizer.scala:1295)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$22.applyOrElse(Optimizer.scala:1292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:258)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:258)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:247)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1292)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1274)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:69)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:78)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3365)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-ba1692c99bc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: 'Detected implicit cartesian product for INNER join between logical plans\\nProject [id#280L, race#281, hitpoints(race#281) AS hp#381, <lambda>() AS land#411]\\n+- LogicalRDD [id#280L, race#281], false\\nand\\nAggregate [land#666], [land#666, avg(cast(hp#381 as bigint)) AS avg(hp)#634, count(id#280L) AS count(id)#635L, avg(id#280L) AS avg(id)#636]\\n+- Project [id#280L, hitpoints(race#281) AS hp#381, <lambda>() AS land#666]\\n   +- LogicalRDD [id#280L, race#281], false\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHcmUsG8rlE2",
        "colab_type": "text"
      },
      "source": [
        "Spark refuses to do cross joins by default. To perform them, we can \n",
        "\n",
        "a) Allow then explicitly:\n",
        "\n",
        "```python\n",
        "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
        "```\n",
        "\n",
        "b) Specify the join criterion\n",
        "\n",
        "```python\n",
        "df4.join(new_df, on='id').show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "raises-exception"
        ],
        "id": "wZqYhoG6rlE3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "3bc65dd8-762f-4e8b-c221-dccc4fe10962"
      },
      "source": [
        "df4.join(result, on='land').show()"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+------+-----+-------+---------+-------+\n",
            "|  land| id|  race|   hp|avg(hp)|count(id)|avg(id)|\n",
            "+------+---+------+-----+-------+---------+-------+\n",
            "| rohan|  0|hobbit|30000|41000.0|       10|    7.4|\n",
            "| rohan|  1|   elf|40000|41000.0|       10|    7.4|\n",
            "| rohan|  2|   elf|40000|41000.0|       10|    7.4|\n",
            "| rohan|  3|   elf|40000|41000.0|       10|    7.4|\n",
            "| rohan|  6|   orc|70000|41000.0|       10|    7.4|\n",
            "| rohan| 10|   elf|40000|41000.0|       10|    7.4|\n",
            "| rohan| 12|   elf|40000|41000.0|       10|    7.4|\n",
            "| rohan| 14|hobbit|30000|41000.0|       10|    7.4|\n",
            "| rohan| 15|hobbit|30000|41000.0|       10|    7.4|\n",
            "| rohan| 19|   elf|40000|41000.0|       10|    7.4|\n",
            "|gondor|  4|   orc|70000|45000.0|       10|   11.6|\n",
            "|gondor|  5|   orc|70000|45000.0|       10|   11.6|\n",
            "|gondor|  7|   elf|40000|45000.0|       10|   11.6|\n",
            "|gondor|  8|hobbit|30000|45000.0|       10|   11.6|\n",
            "|gondor|  9|   elf|40000|45000.0|       10|   11.6|\n",
            "|gondor| 11|hobbit|30000|45000.0|       10|   11.6|\n",
            "|gondor| 13|   elf|40000|45000.0|       10|   11.6|\n",
            "|gondor| 16|   elf|40000|45000.0|       10|   11.6|\n",
            "|gondor| 17|hobbit|30000|45000.0|       10|   11.6|\n",
            "|gondor| 18|   orc|70000|45000.0|       10|   11.6|\n",
            "+------+---+------+-----+-------+---------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMYizxujRY11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "14ce2cb3-bff0-4319-a3b7-a5a5db2f50d5"
      },
      "source": [
        "df4.join(result, on=df4['id'] > result['count(id)']).show()"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+-----+------+------+------------------+---------+-------+\n",
            "| id|  race|   hp|  land|  land|           avg(hp)|count(id)|avg(id)|\n",
            "+---+------+-----+------+------+------------------+---------+-------+\n",
            "|  9|   elf|40000| rohan| rohan|           37500.0|        8|  9.875|\n",
            "| 10|   elf|40000| rohan| rohan|           37500.0|        8|  9.875|\n",
            "| 11|hobbit|30000| rohan| rohan|           37500.0|        8|  9.875|\n",
            "| 12|   elf|40000| rohan| rohan|           37500.0|        8|  9.875|\n",
            "| 13|   elf|40000|gondor| rohan|           37500.0|        8|  9.875|\n",
            "| 14|hobbit|30000| rohan| rohan|           37500.0|        8|  9.875|\n",
            "| 15|hobbit|30000|gondor| rohan|           37500.0|        8|  9.875|\n",
            "| 16|   elf|40000|gondor| rohan|           37500.0|        8|  9.875|\n",
            "| 17|hobbit|30000|gondor| rohan|           37500.0|        8|  9.875|\n",
            "| 18|   orc|70000| rohan| rohan|           37500.0|        8|  9.875|\n",
            "| 19|   elf|40000|gondor| rohan|           37500.0|        8|  9.875|\n",
            "| 13|   elf|40000|gondor|gondor|46666.666666666664|       12|   9.25|\n",
            "| 14|hobbit|30000| rohan|gondor|46666.666666666664|       12|   9.25|\n",
            "| 15|hobbit|30000| rohan|gondor|46666.666666666664|       12|   9.25|\n",
            "| 16|   elf|40000|gondor|gondor|46666.666666666664|       12|   9.25|\n",
            "| 17|hobbit|30000| rohan|gondor|46666.666666666664|       12|   9.25|\n",
            "| 18|   orc|70000|gondor|gondor|46666.666666666664|       12|   9.25|\n",
            "| 19|   elf|40000| rohan|gondor|46666.666666666664|       12|   9.25|\n",
            "+---+------+-----+------+------+------------------+---------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGtznrjGRenN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "62c12638-956b-44fb-d108-81a2b170eca6"
      },
      "source": [
        "df4.join(result, on=df4['id'] > result['count(id)'], how='left').show()"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+-----+------+------+-------+---------+-------+\n",
            "| id|  race|   hp|  land|  land|avg(hp)|count(id)|avg(id)|\n",
            "+---+------+-----+------+------+-------+---------+-------+\n",
            "|  0|hobbit|30000| rohan|  null|   null|     null|   null|\n",
            "|  1|   elf|40000| rohan|  null|   null|     null|   null|\n",
            "|  2|   elf|40000|gondor|  null|   null|     null|   null|\n",
            "|  3|   elf|40000|gondor|  null|   null|     null|   null|\n",
            "|  4|   orc|70000|gondor|  null|   null|     null|   null|\n",
            "|  5|   orc|70000|gondor|  null|   null|     null|   null|\n",
            "|  6|   orc|70000| rohan|  null|   null|     null|   null|\n",
            "|  7|   elf|40000|gondor|  null|   null|     null|   null|\n",
            "|  8|hobbit|30000|gondor|  null|   null|     null|   null|\n",
            "|  9|   elf|40000|gondor|  null|   null|     null|   null|\n",
            "| 10|   elf|40000|gondor|  null|   null|     null|   null|\n",
            "| 11|hobbit|30000|gondor| rohan|36000.0|       10|   11.5|\n",
            "| 11|hobbit|30000|gondor|gondor|50000.0|       10|    7.5|\n",
            "| 12|   elf|40000| rohan| rohan|36000.0|       10|   11.5|\n",
            "| 12|   elf|40000| rohan|gondor|50000.0|       10|    7.5|\n",
            "| 13|   elf|40000|gondor| rohan|36000.0|       10|   11.5|\n",
            "| 13|   elf|40000|gondor|gondor|50000.0|       10|    7.5|\n",
            "| 14|hobbit|30000| rohan| rohan|36000.0|       10|   11.5|\n",
            "| 14|hobbit|30000| rohan|gondor|50000.0|       10|    7.5|\n",
            "| 15|hobbit|30000| rohan| rohan|36000.0|       10|   11.5|\n",
            "+---+------+-----+------+------+-------+---------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "FVLq8C5prlE6",
        "colab_type": "text"
      },
      "source": [
        "#### Digression: Don't work in GoogleColaboratory\n",
        "\n",
        "We can monitor our running jobs and storage used at the Spark Web UI. We can get its url with sc.uiWebUrl.\n",
        "\n",
        "StorageLevels represent how our DataFrame is cached: we can save the results of the computation up to that point, so that if we process several times the same data only the subsequent steps will be recomputed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF3QnD39rlE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2GklEtnrlFB",
        "colab_type": "text"
      },
      "source": [
        "We can erase it with `unpersist`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a74c3eHJrlFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYHfc27QrlFJ",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "Calculate the [z-score](http://www.statisticshowto.com/probability-and-statistics/z-score/) of each employee's hitpoints for their location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbanlQzyrlFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gox9ewy1rlFP",
        "colab_type": "text"
      },
      "source": [
        "1) Calculate the mean and std of hitpoints for each location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mwQs8pYrlFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp7km1qLrlFS",
        "colab_type": "text"
      },
      "source": [
        "2) Annotate each employee with the stats corresponding to their location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lYRtL4crlFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCneaVfRrlFf",
        "colab_type": "text"
      },
      "source": [
        "3) Calculate the z-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bir07D0_rlFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lBXXf-_rlFn",
        "colab_type": "text"
      },
      "source": [
        "Note that we can build more complex boolean conditions for joining, as well as joining on columns that do not have the same name:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyjvnTgqrlFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83DICaI0rlFt",
        "colab_type": "text"
      },
      "source": [
        "### Handling null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih9FCpofrlFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiziuzWkrlFx",
        "colab_type": "text"
      },
      "source": [
        "## SQL querying\n",
        "\n",
        "We need to register our DataFrame as a table in the SQL context in order to be able to query against it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hujnB8F8rlFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-kTYhVXrlF0",
        "colab_type": "text"
      },
      "source": [
        "Once registered, we can perform queries as complex as we want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOHQMYZvrlF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inwnMdakrlF3",
        "colab_type": "text"
      },
      "source": [
        "## Interoperation with Pandas\n",
        "\n",
        "Easy peasy. We can convert a spark DataFrame into a Pandas one, which will `collect` it, and viceversa, which will distribute it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1XqhsGHrlF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4xTmzUrrlGD",
        "colab_type": "text"
      },
      "source": [
        "## Writing out\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf9ypy18rlGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAM4C1k-rlGH",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "Repeat the exercise from the previous notebook, but this time with DataFrames.\n",
        "\n",
        "Get stats for all tickets with destination MAD from `coupons150720.csv`.\n",
        "\n",
        "You will need to extract ticket amounts with destination MAD, and then calculate:\n",
        "\n",
        "1. Total ticket amounts per origin\n",
        "2. Top 10 airlines by average amount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkP5r3XirlGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNW022eDrlGQ",
        "colab_type": "text"
      },
      "source": [
        "1) Extract the fields you need (c0,c1,c2,c3,c4 and c6) into a dataframe with proper names and types\n",
        "\n",
        "Remember, you want to calculate:\n",
        "\n",
        "Total ticket amounts per origin\n",
        "\n",
        "Top 10 airlines by average amount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3Dyy-19rlGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aIdn2NlrlGU",
        "colab_type": "text"
      },
      "source": [
        "2) Total ticket amounts per origin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge6QmGV2rlGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-x2Y_frrlGX",
        "colab_type": "text"
      },
      "source": [
        "3) Top 10 Airlines by average amount\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWSaMYFOrlGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqQM97lsrlGa",
        "colab_type": "text"
      },
      "source": [
        "## Further Reading\n",
        "\n",
        "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/apache-spark-python\n",
        "\n",
        "https://spark.apache.org/docs/2.2.0/sql-programming-guide.html\n",
        "\n",
        "https://ogirardot.wordpress.com/2015/05/29/rdds-are-the-new-bytecode-of-apache-spark/\n",
        "\n",
        "https://stackoverflow.com/questions/36822224/what-are-the-pros-and-cons-of-parquet-format-compared-to-other-formats\n",
        "\n",
        "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf"
      ]
    }
  ]
}